{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f30ffeaa-301b-4bbd-bac4-4b9e01c1eab9",
   "metadata": {},
   "source": [
    "## Figure 8: (Multiple night observations from Figure 4 inputted into Find_Orb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38186b9c-0b0b-4aff-8743-62660d7a5ca5",
   "metadata": {},
   "source": [
    "##### This notebook reproduces the analysis used to generate Figure 8, row 2, implementing the equations presented in Section 2.1 of Maryann et al. (2025) for a sample of 10 asteroids observed at the same timestamp range as used in Figure 4. We selected 10 out of the original 20 objects to ensure that all observations were above the horizon and unaffected by daylight, thus allowing a clean and consistent comparison.\n",
    "\n",
    "##### We apply both the linear model in Equation 1 (column 1 of the final plot of step 18) and the quadratic model in Equation 5 (column 2 of the final plot of step 18) to evaluate how each formulation behaves over long observational arcs, and to determine which model is required for accurate topocentric parallax analysis when compared with Find_Orb. \n",
    "\n",
    "##### In step 19, we convert the Horizons astrometry into MPC-format .obs files for use with Find_Orb, and compare the distances derived from the orbit determination software with those computed directly from Equation 5 in the manuscript's Section 4. We find that over long time spans, the linear term becomes unnecessary, as the asteroid’s apparent motion is effectively non-linear during the five-days, three-observation-per-night observational window."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c527a29-574a-4b66-a05f-88a69b4976f7",
   "metadata": {},
   "source": [
    "##### Step 1: Here we import all the packages used throughout the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3ea2b7-375c-4f9e-8eca-ca8e068b8275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For numerical arrays and dataframes.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For handling plotting and axis formatting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "# For custom subplot layouts\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Tools for fitting functions to data\n",
    "from astropy.modeling import models, fitting\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# For file handling and regular expressions\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# Access to trigonometric functions and constants.\n",
    "import math\n",
    "\n",
    "# For generating random numbers\n",
    "import random\n",
    "\n",
    "# For custom legends\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Text & encoding utilities\n",
    "import unicodedata\n",
    "\n",
    "# Astronomy-specific tools (Astropy)\n",
    "from astropy.time import Time\n",
    "import astropy.units as u\n",
    "from astropy.coordinates import Angle\n",
    "\n",
    "# Date & time utilities\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866aa199-ae4e-4d85-9ce8-bed947ff98bb",
   "metadata": {},
   "source": [
    "##### Step 2: Reading and Extracting the raw data of the 10 asteroids from the NASA JPL Horizon System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701f62d6-a8c4-4a39-8c1d-f31167461a09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# txt_files_folder  = where the Horizons .txt files are stored\n",
    "# csv_output_folder = where the converted .csv files will be saved\n",
    "txt_files_folder  = '/hpc/home/mf342/Maryann et al. 2025/Figure 8/Multiple night'\n",
    "csv_output_folder = '/hpc/home/mf342/Maryann et al. 2025/Figure 8/Multiple night/csv_files'\n",
    "\n",
    "# os.makedirs(): creates the above folder if it does not exist.\n",
    "if not os.path.exists(csv_output_folder):\n",
    "    os.makedirs(csv_output_folder)\n",
    "    print(f\"Created output folder: {csv_output_folder}\")\n",
    "\n",
    "# In the DataFrame's list, the following are the column headers. The blanks denoted by '' correspond to columns that are not required in the analysis. \n",
    "# Date__(UT)__HR:MN = the time and day for each observation. \n",
    "# R.A._(ICRF), DEC__(ICRF) = The RA and DEC data of each observation in decimal format\n",
    "# delta = the distance to the asteroid at each observation in astronomical units (au). We use the Horizons distance to verify the measured topocentric parallax distance in step 7, thereby confirming the measured difference from the true distance. \n",
    "# deldot = the range rate (km/sec). We do not use this column in our analysis. Still, FYI, a positive (+) deldot indicates that an object is moving away from the observer, and a negative (-) deldot indicates that an object is moving towards the observer.\n",
    "columns = ['Date__(UT)__HR:MN', '', '', ' R.A._(ICRF)', 'DEC__(ICRF)', 'delta', 'deldot', '']\n",
    "\n",
    "# ------- We convert the document from a .txt file to a .csv file in the following steps. \n",
    "# pandas.DataFrame: creates a new file with the same column names as seen in the original file.\n",
    "\n",
    "# Loops through every item in the folder to identify relevant files. \n",
    "for file_name in os.listdir(txt_files_folder):\n",
    "\n",
    "    # The file to process must have an extension of .txt\n",
    "    if file_name.endswith('.txt'):\n",
    "        file_path = os.path.join(txt_files_folder, file_name)\n",
    "\n",
    "        # We only want to extract the code between “SOE” (Start of Ephemeris) and “EOE” (End of Ephemeris), since they contain the ephemeris data. \n",
    "        start_marker = \"$$SOE\"\n",
    "        end_marker = \"$$EOE\"\n",
    "\n",
    "        # data_lines: collects the data --> row by row between SOE and EOE.\n",
    "        data_lines = []\n",
    "        try:\n",
    "\n",
    "            # open: opens the file in read mode using 'r'. \n",
    "            # with: ensures that the file is closed properly after reading.\n",
    "            with open(file_path, 'r') as file:\n",
    "\n",
    "                # is_data: A Boolean flag indicating whether a row of data is under analysis and between SOE and EOE, then the data needs to be stored. \n",
    "                is_data = False\n",
    "\n",
    "                # Iterates through every line\n",
    "                for line in file:\n",
    "\n",
    "                   # When a row of lines contains SOE (the start marker), 'is_data' is set to True, and beyond this line, data will be captured.\n",
    "                    if start_marker in line:\n",
    "                        is_data = True\n",
    "\n",
    "                    # When a row of lines contains EOE (the end marker), 'is_data' is set to False, and beyond this line, data will not be captured.\n",
    "                    elif end_marker in line:\n",
    "                        is_data = False\n",
    "                   \n",
    "                    # In the case that neither marker is found, and 'is_data' is set at true, it means that data capturing is still on-going, and hasn't reached EOE. \n",
    "                    elif is_data:\n",
    "\n",
    "                        # .strip(): removes whitespaces and sends lines to data_lines.\n",
    "                        data_lines.append(line.strip())\n",
    "            \n",
    "            # Each line of ephemeris data that is captured is split at the commas to form string values so that they are in the form of rows and columns (lists of lists) for the csv file. \n",
    "            data = [line.split(\",\") for line in data_lines]\n",
    "\n",
    "            # max_columns: searches for the longest row and its length.\n",
    "            max_columns = max(len(row) for row in data)\n",
    "\n",
    "            # Short rows are padded with empty strings at locations that do not have data, to represent the same length as the longest row. This ensures that the DataFrame is created without misaligned columns.       \n",
    "            data = [row + [''] * (max_columns - len(row)) for row in data]\n",
    "\n",
    "            # Creates a DataFrame and assigns the appropriate column headers.\n",
    "            df = pd.DataFrame(data, columns=columns[:max_columns])\n",
    "\n",
    "            # os.path.basename(file_path): extracts the filename\n",
    "            # os.path.splitext(...)[0]:    removes the .txt file extension so that a new .csv extension can be added\n",
    "            base_name = os.path.splitext(file_name)[0]\n",
    "            csv_file_path = os.path.join(csv_output_folder, f'{base_name}.csv')\n",
    "            df.to_csv(csv_file_path, index=False)\n",
    "            print(f\"CSV file saved: {csv_file_path}\")\n",
    "            \n",
    "        # Error handling: gives notifications if a file is not found or if there are unexpected errors.\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38174e2-9e68-4b57-af47-be0dd51410fc",
   "metadata": {},
   "source": [
    "##### Step 3: From the full ephemeris dataset for each of the 10 asteroids, which spans a complete 24-hour period with hourly sampling, we select only the rows corresponding to nighttime observations. This ensures that all retained measurements fall within the observable window and are not affected by the horizon limit. Note that the specific timestamps used here differ slightly from those shown in Figure 4; however, the selected observing window remains the same, spanning 8:00 pm to 5:00 am."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195a2ead-cf8e-4441-bf9a-f63c441434cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The three observations per night are saved in a newly created .csv file. \n",
    "csv_files_path = '/hpc/home/mf342/Maryann et al. 2025/Figure 8/Multiple night/csv_files'\n",
    "extracted_files_path = '/hpc/home/mf342/Maryann et al. 2025/Figure 8/Multiple night/csv_files/extracted_files'\n",
    "\n",
    "# The following are the rows that need to be extracted.\n",
    "rows_to_extract = [0, 4, 31,                   \n",
    "                   32, 33, 34,                  \n",
    "                   66, 67, 68,                  \n",
    "                   97, 98, 99,                  \n",
    "                   129, 130, 131]\n",
    "\n",
    "os.makedirs(extracted_files_path, exist_ok=True)\n",
    "\n",
    "for file_name in os.listdir(csv_files_path):\n",
    "    if file_name.endswith('.csv'):  \n",
    "        file_path = os.path.join(csv_files_path, file_name)\n",
    "        csv_data = pd.read_csv(file_path)\n",
    "        extracted_data = csv_data.iloc[rows_to_extract]\n",
    "        \n",
    "        base_name = os.path.splitext(file_name)[0]\n",
    "        output_csv = os.path.join(extracted_files_path, f'{base_name}.csv')\n",
    "        extracted_data.to_csv(output_csv, index=False)\n",
    "        \n",
    "        print(f\"Extracted rows saved to: {output_csv}\")\n",
    "        print(extracted_data.to_string(index=True))\n",
    "        print(\"\\n\" + \"-\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30be0c50-1b17-4461-9585-fe6c53e15c42",
   "metadata": {},
   "source": [
    "##### Step 4: Verify that the columns with data that will be analyzed have been properly copied to the new .csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c395f77c-f29d-48a6-8f22-87afd95bde97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_horizons_file(file_path):\n",
    "    try:\n",
    "        # Read the .csv file\n",
    "        data = pd.read_csv(file_path)\n",
    "\n",
    "        # Shorten the original names of the columns\n",
    "        column_mapping = {\n",
    "            'Date__(UT)__HR:MN': 'JDUT',\n",
    "            ' R.A._(ICRF)': 'RA_ICRF',  \n",
    "            'DEC__(ICRF)': 'DEC_ICRF',\n",
    "            'delta': 'delta'}\n",
    "        data = data.rename(columns=column_mapping)\n",
    "\n",
    "        # The four columns required for curve fitting.\n",
    "        required_columns = ['JDUT', 'RA_ICRF', 'DEC_ICRF', 'delta']\n",
    "\n",
    "        # Check that all required columns are present in the DataFrame.\n",
    "        # If any are missing, raise an error to alert the user.\n",
    "        if not all(col in data.columns for col in required_columns):\n",
    "            raise ValueError(f\"Missing one or more required columns in {file_path}\")\n",
    "\n",
    "        # Return a DataFrame containing only the needed columns in the correct order.\n",
    "        return data[required_columns]\n",
    "    \n",
    "    except Exception as e:\n",
    "        # If anything goes wrong, print an error message including the filename.\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "        # Return an empty DataFrame with the correct column structure. \n",
    "        return pd.DataFrame(columns=['JDUT', 'RA_ICRF', 'DEC_ICRF', 'delta'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5673e0d0-c633-4567-8ebb-ec1e1c666dac",
   "metadata": {},
   "source": [
    "##### Step 5: Addition of a randomly generated 20 milliarcseconds (mas) Gaussian noise to each observation in the RA and DEC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784ba67b-e9e8-45c7-bdbb-fe1d039dd676",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# input_file:  The original file path with RA and DEC. \n",
    "# output_file: The new file path with noise added in the RA and DEC. \n",
    "input_folder  = Path(\"/hpc/home/mf342/Maryann et al. 2025/Figure 8/Multiple night/csv_files/extracted_files/\")\n",
    "output_folder = input_folder / \"with_uncertainties\"\n",
    "\n",
    "# The columns that need noise addition in the original file. \n",
    "output_folder.mkdir(parents=True, exist_ok=True)\n",
    "RA_COL  = \" R.A._(ICRF)\"      \n",
    "DEC_COL = \"DEC__(ICRF)\"\n",
    "\n",
    "# Horizons does not provide measurement uncertainties in RA and DEC. So, we randomly add 20 mas of noise to mimic real astrometric errors.\n",
    "# 20 mas converted to arcseconds. 1 mas = 0.001 arcsec and 20 mas = 0.020 arcsec\n",
    "# The noise is Gaussian-distributed, consistent with typical astrometric uncertainties.\n",
    "# 20 mas was chosen since it is a realistic uncertainty approximation based on typical CCD astrometric performance.\n",
    "SIGMA_ARCSEC = 0.020       \n",
    "\n",
    "# converting arcseconds into degrees because RA/DEC in the original Horizons file is in degrees (1 degree = 3600\"). \n",
    "SIGMA_DEG    = SIGMA_ARCSEC / 3600.0  \n",
    "\n",
    "# A random generator. seed=42 ensures reproducible noise, where every run of the code ensures that the noise to the observations stays the same. \n",
    "rng = np.random.default_rng(seed=42)\n",
    "\n",
    "for csv_file in input_folder.glob(\"*.csv\"):\n",
    "    # Reads the .csv into a DataFrame, where each row is an observation.\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # A safety check: If RA or DEC columns are missing, stop the code and show an error.        \n",
    "    if RA_COL not in df.columns or DEC_COL not in df.columns:\n",
    "        print(f\"Skipping {csv_file.name}, RA/DEC not found\")\n",
    "        continue\n",
    "\n",
    "    # Extract the RA and DEC columns into a NumPy array\n",
    "    ra  = df[RA_COL].to_numpy(dtype=float)\n",
    "    dec = df[DEC_COL].to_numpy(dtype=float)\n",
    "\n",
    "    # Generate one random noise value per RA and DEC observation, where mean = 0, standard deviation = 20 mas in degrees, size = number of observations (48).\n",
    "    ra_noise  = rng.normal(0.0, SIGMA_DEG, size=len(df))\n",
    "    dec_noise = rng.normal(0.0, SIGMA_DEG, size=len(df))\n",
    "\n",
    "    # Adds noise to each RA. \n",
    "    # % 360.0: wraps RA so that values remain within 0–360 degrees. \n",
    "    ra_new  = (ra + ra_noise) % 360.0\n",
    "\n",
    "    # Adds noise to each DEC. \n",
    "    # Limits the DEC at -90 and +90, since it cannot exceed the poles. \n",
    "    dec_new = np.clip(dec + dec_noise, -90.0, 90.0)\n",
    "\n",
    "    # Printing a readable header to see the comparison. \n",
    "    print(f\"\\n=== {csv_file.name} ===\")\n",
    "    print(\"Obs |     Original_RA     Original_DEC |    RA_noise(deg)   DEC_noise(deg) |   RA_noise(mas)   DEC_noise(mas) |          New_RA          New_DEC\")\n",
    "    print(\"----+------------------+----------------+----------------------------------+----------------------------------+---------------------------------\")\n",
    "    for i in range(len(df)):\n",
    "        print(f\"{i+1:3d} | \"\n",
    "              f\"{ra[i]:16.8f} {dec[i]:15.8f} | \"\n",
    "              f\"{ra_noise[i]:16.8f} {dec_noise[i]:16.8f} | \"\n",
    "              f\"{ra_noise[i]*3600*1000:16.2f} {dec_noise[i]*3600*1000:15.2f} | \"\n",
    "              f\"{ra_new[i]:16.8f} {dec_new[i]:16.8f}\")\n",
    "\n",
    "    # In the newly created .csv file, the original Horizons .csv file is now replaced with 20mas noise in the RA and DEC.\n",
    "    df[RA_COL]  = ra_new\n",
    "    df[DEC_COL] = dec_new\n",
    "    out_file = output_folder / csv_file.name\n",
    "    df.to_csv(out_file, index=False)\n",
    "    print(f\"Saved {out_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a34966-12d0-4861-8a05-28b370d72261",
   "metadata": {},
   "source": [
    "##### Step 6: Adding the topocentric parallax distance from Find_Orb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a4a3b0-431d-48cc-84de-37786139a77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we load the data for each of the 12 asteroids from the .csv file, which has the results of the distance measurement for each observation from Find_Orb. \n",
    "def load_true_delta_lookup_from_wide_csv(path_to_wide_csv):\n",
    "    df = pd.read_csv(path_to_wide_csv)\n",
    "    numeric = df.select_dtypes(include=\"number\")\n",
    "    col_means = numeric.mean(numeric_only=True)\n",
    "    lookup = {}\n",
    "    for col, val in col_means.items():\n",
    "        key = col if str(col).endswith(\".csv\") else f\"{col}.csv\"\n",
    "        lookup[key] = float(val)\n",
    "    return lookup\n",
    "\n",
    "# The location of the file. \n",
    "TRUTH_WIDE_CSV = \"/hpc/home/mf342/Maryann et al. 2025/Figure 8/Multiple night/Multiple Night.csv\" \n",
    "true_delta_lookup = load_true_delta_lookup_from_wide_csv(TRUTH_WIDE_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3825e4-c275-4040-b7cc-e8e24f0761f6",
   "metadata": {},
   "source": [
    "##### Step 7: Distance measurement using Eqs. 1 and 5 in Section 2.1 in Maryann et al. 2025."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdae6258-1cb5-446d-a459-38a837965e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance_in_au_with_quadratic(file_path, true_delta_lookup):\n",
    "\n",
    "    # Read the Horizons .csv file and return only the needed columns\n",
    "    data = read_horizons_file(file_path)\n",
    "\n",
    "    # Extract columns as NumPy arrays for convenience\n",
    "    JD    = data['JDUT'].values\n",
    "    RA    = data['RA_ICRF'].values\n",
    "    DEC   = data['DEC_ICRF'].values\n",
    "    delta = data['delta'].values\n",
    "\n",
    "    # cos(DEC): evaluated at the first observation, and used in Equation 3 to compute the correction needed to measure topocentric parallax distance\n",
    "    DEC_first_row = math.radians(DEC[0])\n",
    "\n",
    "    # Mean of the true distance taken from Horizons\n",
    "    mean_delta    = float(np.mean(delta))\n",
    "\n",
    "    # Adds the data from Find_Orb that is stored in he csv file. \n",
    "    fname    = os.path.basename(file_path)\n",
    "    Find_Orb = true_delta_lookup.get(fname, np.nan)\n",
    "\n",
    "    # Normalize RA and JD for numerical stability in the fit\n",
    "    # Subtracting the minimum removes any large constant offset\n",
    "    RA_min = RA.min()\n",
    "    RA_normalized = RA - RA_min\n",
    "    JD_min = JD.min()\n",
    "    JD_normalized = JD - JD_min\n",
    "\n",
    "    #### Polynimial (Quadratic) Fitting: Equation 5 ################################################\n",
    "\n",
    "    # Equation: Am*sin(2πx + B) + Cx + D + Ex²\n",
    "    \n",
    "    # Sinusoidal term --> Am * np.sin(2 * np.pi * x + B) is the Earth's rotation (x = time; Am = amplitude of the signal, also known as the parallax amplitude/signal; B = phase shift). \n",
    "    # linear + quadratic terms --> C * x + D + E * x**2 is the asteroids non-linear motion (Cx = asteroids linear motion showing a constant angular velocity; D = RA offset; E x^2 = asteroids non-linear motion due to changing geometry and orbital acceleration)\n",
    "    # fit_function_polynomial defines a Python function for Eq. 5\n",
    "    \n",
    "    def fit_function_polynomial(x, A, B, C, D, E):\n",
    "        return A * np.sin(2 * np.pi * x + B) + C * x + D + E * x**2\n",
    "\n",
    "    # Fits the polynomial model to the normalized RA vs normalized time\n",
    "    # p0 = initial guesses for Am, B, C, D, and E (Best-fit parameters)\n",
    "    # curve_fit() from scipy.optimize --> performs the least-squares fit. It finds the best fit value for Am, B, C, D, and E that fit into the RA data with the 20mas noise.\n",
    "    # params_sine = a NumPy array of best-fit parameter values\n",
    "    # covariance_sine = 5×5 covariance matrix describing the uncertainties and correlations for Am, B, C, D, and E.\n",
    "    \n",
    "    params_poly, covariance_sine = curve_fit(\n",
    "        fit_function_polynomial, JD_normalized, RA_normalized,\n",
    "        p0=[1, 0, 0, 0, 0], maxfev=15000)\n",
    "    A, B, C, D, E = params_poly\n",
    "\n",
    "    # Convert fitted amplitude Am from degrees to radians\n",
    "    A_radians_poly = np.radians(A)\n",
    "\n",
    "    # Uncertainties in Equation 5. \n",
    "    # It is important for understanding the error propagation. \n",
    "    \n",
    "    # np.diag(covariance_sine): extracts diagonal elements and the variances of Am, B, C, D, and E.\n",
    "    # np.sqrt(...):             takes the square root and gives the standard deviations (1σ uncertainties).\n",
    "    sine_uncertainties = np.sqrt(np.diag(covariance_sine))\n",
    "    \n",
    "    # Converts the uncertainty of the fitted amplitude (Am) from degrees to radians\n",
    "    amplitude_uncertainty_sine = np.radians(sine_uncertainties[0])\n",
    "\n",
    "    #### Linear Fitting: Equation 1 ################################################\n",
    "\n",
    "    # Equation: Am*sin(2πx + B) + Cx + D\n",
    "    # Here, the asteroid’s motion is approximated as linear in RA. The meaning of each component is the same as seen in Eq. 5 above, and the only difference is the removal of the polynomial/quadratic term from the analysis and the equation. \n",
    "  \n",
    "    def fit_function_linear(x, A, B, C, D):\n",
    "        return A * np.sin(2 * np.pi * x + B) + C * x + D\n",
    "\n",
    "    # Fit the linear model to the same normalized data    \n",
    "    params_lin, covariance_linear = curve_fit(\n",
    "        fit_function_linear, JD_normalized, RA_normalized,\n",
    "        p0=[1, 0, 0, 0], maxfev=15000)\n",
    "    A_lin, B_lin, C_lin, D_lin = params_lin\n",
    "\n",
    "    # Convert fitted amplitude (Am_lin) to radians\n",
    "    A_lin_radians = np.radians(A_lin)\n",
    "\n",
    "    # 1σ uncertainties for the linear model parameters\n",
    "    linear_uncertainties = np.sqrt(np.diag(covariance_linear))\n",
    "\n",
    "    # Convert the uncertainty of the fitted amplitude (Am_lin) from degrees to radians\n",
    "    amplitude_uncertainty_linear = np.radians(linear_uncertainties[0])\n",
    "\n",
    "    #### Constants ################################################\n",
    "\n",
    "    # Earth's radius in km    \n",
    "    radius_of_earth_km = 6371\n",
    "\n",
    "    # 1 au in km\n",
    "    km_to_au = 149_597_870.7\n",
    "\n",
    "    # Observatory latitude (deg)\n",
    "    ##### Note: The Cerro Tololo latitude used in the published manuscript (30.1732° S) differs slightly from the precise geodetic latitude reported by JPL Horizons (−30.1691165°). \n",
    "    #####       The code released here uses the Horizons value. This difference has a negligible effect on the topocentric parallax calculation and does not affect any results or conclusions in the paper.\n",
    "    \n",
    "    latitude_deg = 30.1691165\n",
    "    latitude_rad = math.radians(latitude_deg)\n",
    "\n",
    "     # -------------------------------------------------------\n",
    "    # -------------------------------------------------------\n",
    "    #### Equation 3 ==> computed for both the linear and quadratic Fit ###############################################\n",
    "    # Since the measurements do not have an observatory's latitude or the DEC of the observations, topocentric parallax is not accounted for in the calculations. To fix this, we need two adjustments as follows: \n",
    "    \n",
    "    # Latitude of the Observatory (Correction 1)\n",
    "    # The observatory taking the observations of all the 20 asteroids is the Cerro Tololo Observatory, La Serena, (code: 807) \n",
    "    # The observatory is situated at an altitude of 2388 m\n",
    "    # The observatory is in the southern hemisphere of the Earth at a latitude of 30.169° S. \n",
    "    # We convert the latitude from degrees to radians. \n",
    "    # Taken from the Horizons file: Center geodetic: 289.1941, -30.1691165, 2.38888  {E-lon(deg), Lat(deg), Alt(km)}\n",
    "\n",
    "    # Declination (DEC) of the observations (Correction 2)\n",
    "    # Since the DEC of the observations is over a single day, the change in the degrees is very small, and so we assume a single DEC value fixed at the first observation.\n",
    "    distance_km_poly             = (math.cos(DEC_first_row) / math.cos(latitude_rad)) * A_radians_poly\n",
    "    distance_uncertainty_km_poly = (math.cos(DEC_first_row) / math.cos(latitude_rad)) * amplitude_uncertainty_sine\n",
    "\n",
    "    distance_km_lin              = (math.cos(DEC_first_row) / math.cos(latitude_rad)) * A_lin_radians\n",
    "    distance_uncertainty_km_lin  = (math.cos(DEC_first_row) / math.cos(latitude_rad)) * amplitude_uncertainty_linear\n",
    "    # -------------------------------------------------------\n",
    "    # -------------------------------------------------------\n",
    "    ### Equation 4 ==> computed for both the linear and quadratic Fit ################################################\n",
    "    # Distance measurement concerning the center of the Earth. \n",
    "    final_distance_km_poly                = radius_of_earth_km / distance_km_poly\n",
    "    final_distance_uncertainty_km_poly    = radius_of_earth_km * distance_uncertainty_km_poly / (distance_km_poly**2)\n",
    "    \n",
    "    final_distance_km_lin                 = radius_of_earth_km / distance_km_lin\n",
    "    final_distance_uncertainty_km_lin     = radius_of_earth_km * distance_uncertainty_km_lin  / (distance_km_lin**2)\n",
    "    # -------------------------------------------------------\n",
    "    # -------------------------------------------------------\n",
    "    #### Asteroid Distance (au) conversion from km ###################################\n",
    "    final_distance_au_poly             = final_distance_km_poly / km_to_au\n",
    "    final_distance_uncertainty_au_poly = final_distance_uncertainty_km_poly / km_to_au\n",
    "    \n",
    "    final_distance_au_lin              = final_distance_km_lin / km_to_au\n",
    "    final_distance_uncertainty_au_lin  = final_distance_uncertainty_km_lin / km_to_au\n",
    "    # -------------------------------------------------------\n",
    "    # -------------------------------------------------------\n",
    "    \n",
    "    #### Percentage Difference between True and Measured ###########\n",
    "    \n",
    "    # Guard against division by zero\n",
    "    if mean_delta == 0 or final_distance_au_poly == 0:\n",
    "        percentage_poly = 0.0\n",
    "        percentage_lin  = 0.0\n",
    "    else:\n",
    "        percentage_poly = abs(abs(final_distance_au_poly) - abs(mean_delta)) / ((abs(final_distance_au_poly) + abs(mean_delta)) / 2) * 100.0\n",
    "        percentage_lin  = abs(abs(final_distance_au_lin)  - abs(mean_delta)) / ((abs(final_distance_au_lin)  + abs(mean_delta))  / 2) * 100.0\n",
    "\n",
    "    # Returns distances and uncertainties (Eq. 5 and Eq. 1), true mean delta, percentage differences, and the fit parameters for further inspection.\n",
    "    return (\n",
    "        final_distance_au_poly, final_distance_uncertainty_au_poly,\n",
    "        final_distance_au_lin,  final_distance_uncertainty_au_lin,\n",
    "        mean_delta, \n",
    "        percentage_poly, percentage_lin,\n",
    "        params_poly, params_lin, Find_Orb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d9b446-d3f8-48ab-bfc7-019d4f65fcef",
   "metadata": {},
   "source": [
    "##### Step 8: Process every Horizons file and prints the distance measurement to each asteroid as well as the percentage difference between the measured and Horizon distance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eec3e2-7edd-4428-b265-20822b58a229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function processes every asteroid .csv file in a folder\n",
    "def process_all_asteroid_files(folder_path, true_delta_lookup):\n",
    "    \n",
    "    # This list will store the full results for all asteroids.\n",
    "    results = []\n",
    "\n",
    "    # Loop through all files inside the provided folder.\n",
    "    for file_name in os.listdir(folder_path):\n",
    "\n",
    "        # Construct the full path to the file.\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        # Only process files that: (1) Are real files; (2) End with .csv \n",
    "        if os.path.isfile(file_path) and file_name.endswith('.csv'):\n",
    "            try:\n",
    "\n",
    "                # calls the function that reads the Horizons file, and fits the RA data for both Equations 1 and 5. Later, we compute the distance and uncertainties for the 10 asteroids and the fractional distance scatter.  \n",
    "                result = compute_distance_in_au_with_quadratic(file_path, true_delta_lookup)\n",
    "                \n",
    "                # Stores the file names and unpacks the returned values into the resulting list.\n",
    "                results.append((file_name, *result))\n",
    "\n",
    "                # Extracts specific values for user readability.\n",
    "                mean_delta            = result[4]\n",
    "                distance_in_au_poly   = result[0]\n",
    "                distance_in_au_lin    = result[2]\n",
    "                percentage_poly       = result[5]\n",
    "                percentage_lin        = result[6]\n",
    "                Find_Orb              = result[9]\n",
    "\n",
    "                # Prints a well-formatted summary line for each asteroid.\n",
    "                print(f\"{file_name:<17} | Mean (au): {float(mean_delta):>8.4f} | \"\n",
    "                      f\"Poly (au): {float(abs(distance_in_au_poly)):>8.4f} | \"\n",
    "                      f\"Poly (%): {float(abs(percentage_poly)):>8.4f} | \"\n",
    "                      f\"Lin (au): {float(abs(distance_in_au_lin)):>8.4f} | \"\n",
    "                      f\"Lin (%): {float(abs(percentage_lin)):>8.4f} | \"\n",
    "                      f\"Find_Orb (au): {float(abs(Find_Orb)) if not np.isnan(Find_Orb) else float('nan'):>8.4f}\")\n",
    "\n",
    "            # Error handling: bad files are printed for user notification. This is to make sure that the entire analysis does not crash.\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "    # Returns the full list of results so it may be turned into a DataFrame.\n",
    "    return results\n",
    "\n",
    "# This is the path where the results of the distance measurement, uncertainties, the fit, and more are saved in a new .csv file. \n",
    "asteroids_folder = \"/hpc/home/mf342/Maryann et al. 2025/Figure 8/Multiple night/csv_files/extracted_files/with_uncertainties\"\n",
    "all_results = process_all_asteroid_files(asteroids_folder, true_delta_lookup)\n",
    "\n",
    "results_df = pd.DataFrame(all_results, columns=[\n",
    "    'File Name', \n",
    "    'Distance in AU (Poly)', 'Uncertainty (Poly)', \n",
    "    'Distance in AU (Lin)',  'Uncertainty (Lin)', \n",
    "    'Mean Delta', \n",
    "    'percentage_poly', 'percentage_lin',\n",
    "    'Fit Parameters (Poly)', 'Fit Parameters (Lin)', 'Find_Orb'])\n",
    "results_df.to_csv('asteroid_distances_poly_linear.csv', index=False)\n",
    "print(\"Results saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e478a0bd-696d-4755-bef8-9680cc109243",
   "metadata": {},
   "source": [
    "##### Step 9: Extract the true distance from Horizons and the measured distance from Eqs. 1 and 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6533a8a-247c-4b13-99f3-5bb26ea44460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the true distances (Horizons delta column), and the fitted distances.\n",
    "# abs(): ensures that all values are positive before comparison.\n",
    "\n",
    "true_values             = abs(results_df['Mean Delta'])\n",
    "cosine_term_lin_values  = abs(results_df['Distance in AU (Lin)'])\n",
    "cosine_term_poly_values = abs(results_df['Distance in AU (Poly)'])\n",
    "Find_Orb                = abs(results_df['Find_Orb'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54b5bc4-de4a-4b5f-8587-a8577d6d772f",
   "metadata": {},
   "source": [
    "##### Step 10: Fractional residual = (fitted_distance – true_distance) / true_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff853a5-2c35-4244-b573-b8740dbf6058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measures how far the fitted distance deviates from the true one.\n",
    "\n",
    "residuals_lin             = (cosine_term_lin_values - true_values)  / true_values\n",
    "residuals_poly            = (cosine_term_poly_values - true_values) / true_values\n",
    "residuals_Horizon_FindOrb = (Find_Orb - true_values)                / true_values\n",
    "\n",
    "# Print the residuals asteroid-by-asteroid for inspection.\n",
    "asteroid_names = results_df['File Name'].tolist()\n",
    "residuals_df = pd.DataFrame({\n",
    "    \"Asteroid\": asteroid_names,\n",
    "    \"residuals_lin\": residuals_lin,\n",
    "    \"residuals_poly\": residuals_poly,\n",
    "    \"residuals_Horizon_FindOrb\": residuals_Horizon_FindOrb,\n",
    "    \"true_values\": true_values})\n",
    "\n",
    "output_csv = \"asteroid_residuals.csv\"\n",
    "residuals_df.to_csv(output_csv, index=False)\n",
    "print(f\"Residuals data saved to {output_csv}\")\n",
    "\n",
    "print(f\"{'Asteroid':>20} {'residuals_lin':>20} {'residuals_poly':>20} \"\n",
    "      f\"{'residuals_Horizon_FindOrb':>35} {'true_values':>35}\")\n",
    "print(\"-\" * 150)\n",
    "\n",
    "try:\n",
    "    iter(true_values)\n",
    "    true_values_iter = true_values\n",
    "except TypeError:\n",
    "    true_values_iter = [true_values] * len(asteroid_names)\n",
    "\n",
    "for name, r_lin, r_poly, r_hf, tv in zip(\n",
    "    asteroid_names, residuals_lin, residuals_poly, residuals_Horizon_FindOrb, true_values_iter\n",
    "):\n",
    "    print(f\"{name:>20} {r_lin:>20f} {r_poly:>20f} {r_hf:>35f} {tv:>35f}\")\n",
    "\n",
    "    residuals_Maryann_FindOrb = (Find_Orb - true_values)                / true_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe7fe57-e828-47d3-86cc-7fe7f5e7ec48",
   "metadata": {},
   "source": [
    "##### Step 11: Statistical properties of the residuals: Standard Deviation (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc936c21-1162-4ac3-abe4-b20a9b5ad395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) std  → scatter (precision)\n",
    "\n",
    "sigma_lin                 = np.std(residuals_lin)\n",
    "sigma_poly                = np.std(residuals_poly)\n",
    "sigma_Horizon_FindOrb     = np.std(residuals_Horizon_FindOrb)\n",
    "sigma_Maryann_FindOrb     = np.std(residuals_Maryann_FindOrb)\n",
    "\n",
    "print(f\"sigma_lin: {sigma_lin:.4f}\")\n",
    "print(f\"sigma_poly: {sigma_poly:.4f}\")\n",
    "print(f\"sigma_Horizons_FindOrb: {sigma_Horizon_FindOrb:.4f}\")\n",
    "print(f\"sigma_Maryann_FindOrb: {sigma_Maryann_FindOrb:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b95369-780d-41ad-833b-947b8b978a22",
   "metadata": {},
   "source": [
    "##### Step 12: Statistical properties of the residuals: Mean (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1daf36-c477-4821-b595-71f85e2d4604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) mean → bias (systematic offset)\n",
    "\n",
    "lin_mean                 = np.mean(residuals_lin)\n",
    "poly_mean                = np.mean(residuals_poly)\n",
    "Horizon_FindOrb_mean     = np.mean(residuals_Horizon_FindOrb)\n",
    "Maryann_FindOrb_mean     = np.mean(residuals_Maryann_FindOrb)\n",
    "\n",
    "print(f\"lin_mean: {lin_mean:.4f}\")\n",
    "print(f\"poly_mean: {poly_mean:.4f}\")\n",
    "print(f\"Horizons_FindOrb_mean: {Horizon_FindOrb_mean:.4f}\")\n",
    "print(f\"Maryann_FindOrb_mean: {Maryann_FindOrb_mean:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f6ad99-7bcf-47ab-a69b-460329ca17d8",
   "metadata": {},
   "source": [
    "##### Step 13: Statistical properties of the residuals: Median (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b33ee5-b84b-41eb-bbb3-f2ba59b01317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) median → robust central value\n",
    "\n",
    "lin_median                 = np.median(residuals_lin)\n",
    "poly_median                = np.median(residuals_poly)\n",
    "Horizon_FindOrb_median     = np.median(residuals_Horizon_FindOrb)\n",
    "Maryann_FindOrb_median     = np.median(residuals_Maryann_FindOrb)\n",
    "\n",
    "print(f\"lin_median: {lin_median:.4f}\")\n",
    "print(f\"lin_median: {poly_median:.4f}\")\n",
    "print(f\"Horizons_FindOrb_median: {Horizon_FindOrb_median:.15f}\")\n",
    "print(f\"Maryann_FindOrb_median: {Maryann_FindOrb_median:.15f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39df6d1b-28d9-4e03-8d83-fb08f27948c7",
   "metadata": {},
   "source": [
    "##### Step 14: Steps before computing the reduced chi-square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91be76f5-43e4-4b97-b843-7886a90dbc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert percentage uncertainties into fractional uncertainties, then convert them into absolute distance uncertainties for weighting chi-squared.\n",
    "results_df['Lin Percentage Error'] = results_df['percentage_lin'] / 100\n",
    "results_df['Poly Percentage Error'] = results_df['percentage_poly'] / 100\n",
    "\n",
    "# Absolute uncertainties --> σ = fractional_error × distance\n",
    "uncertainty_cosine_lin = abs(results_df['Lin Percentage Error'] * cosine_term_lin_values)\n",
    "uncertainty_cosine_poly = abs(results_df['Poly Percentage Error'] * cosine_term_poly_values)\n",
    "\n",
    "# Relative uncertainties for chi-squared weighting\n",
    "residuals_uncertainty_lin = abs(uncertainty_cosine_lin / true_values)\n",
    "residuals_uncertainty_poly = abs(uncertainty_cosine_poly / true_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7509ce8d-a4b6-487a-a53a-7159145f1488",
   "metadata": {},
   "source": [
    "##### Step 15: Chi-squared: χ² = Σ (residual² / uncertainty²)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931c7528-c7e0-41e9-9155-f6d2d2d1182e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_lin = np.sum((residuals_lin**2) / (residuals_uncertainty_lin**2))\n",
    "chi2_poly = np.sum((residuals_poly**2) / (residuals_uncertainty_poly**2))\n",
    "print(f\"Chi-squared (Linear): {chi2_lin:.4f}\")\n",
    "print(f\"Chi-squared (Polynomial): {chi2_poly:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cc4675-b7a8-421f-b1aa-f54ec4c1ec54",
   "metadata": {},
   "source": [
    "##### Step 16: Compute Reduced Chi-squared: χ² = χ² / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa60325-f338-4993-8655-a69ba6003cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretation:\n",
    "    #   ~1  → good fit\n",
    "    #   >1  → poor fit \n",
    "    #   <1  → uncertainties overestimated\n",
    "\n",
    "N_lin = len(residuals_lin)  \n",
    "N_poly = len(residuals_poly)\n",
    "\n",
    "red_chi2_lin = chi2_lin / N_lin\n",
    "red_chi2_poly = chi2_poly / N_poly\n",
    "\n",
    "print(f\"Reduced Chi-squared (Linear): {red_chi2_lin:.2f}\")\n",
    "print(f\"Reduced Chi-squared (Polynomial): {red_chi2_poly:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf0320a-e346-40af-806a-a0312984083e",
   "metadata": {},
   "source": [
    "##### Step 17: Restrict the sample to near-Earth asteroids (d < 0.3 au), since these objects are most relevant for planetary-defense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a2be74-ceea-47ec-b785-ab5ac9b6d5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STD of asteroids below 0.3 au\n",
    "below_03_mask_residuals = (true_values < 0.3)\n",
    "\n",
    "filtered_file_names = results_df['File Name'][below_03_mask_residuals]\n",
    "print(filtered_file_names.tolist())\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "filtered_residuals_lin = residuals_lin[below_03_mask_residuals]\n",
    "std_dev_residuals_below_03 = np.std(filtered_residuals_lin)\n",
    "print(f\"Standard Deviation of Linear Fit residuals below 0.3 AU: {std_dev_residuals_below_03:.4f}\")\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "filtered_residuals_poly = residuals_poly[below_03_mask_residuals]\n",
    "std_dev_residuals_below_03_poly = np.std(filtered_residuals_poly)\n",
    "print(f\"Standard Deviation of Polynomial Fit residuals below 0.3 AU: {std_dev_residuals_below_03_poly:.4f}\")\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "filtered_residuals_Horizons_FindOrb = residuals_Horizon_FindOrb[below_03_mask_residuals]\n",
    "std_dev_residuals_below_03_Horizons_FindOrb = np.std(filtered_residuals_Horizons_FindOrb)\n",
    "print(f\"Standard Deviation of Horizons vs. Find_Orb residuals below 0.3 AU: {std_dev_residuals_below_03_Horizons_FindOrb:.4f}\")\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "filtered_residuals_Maryann_FindOrb = residuals_Maryann_FindOrb[below_03_mask_residuals]\n",
    "std_dev_residuals_below_03_Maryann_FindOrb = np.std(filtered_residuals_Maryann_FindOrb)\n",
    "print(f\"Standard Deviation of Maryann vs. Find_Orb residuals below 0.3 AU: {std_dev_residuals_below_03_Maryann_FindOrb:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091c2f35-8d2b-45c4-8c13-d574b110d56b",
   "metadata": {},
   "source": [
    "##### Step 18: Plot the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246819a2-ff1a-4b4d-af55-9b7a58ca54d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25, 5))\n",
    "gs = GridSpec(\n",
    "    1, 3,\n",
    "    width_ratios=[1, 1, 1],\n",
    "    height_ratios=[1.5])\n",
    "\n",
    "colors = plt.cm.tab20(np.linspace(0, 1, len(results_df)))\n",
    "\n",
    "# ================================================================================================================================\n",
    "\n",
    "# Bottom-left: Residuals for Linear fit\n",
    "ax1 = plt.subplot(gs[0, 0])\n",
    "for i, color in enumerate(colors):\n",
    "    ax1.errorbar(\n",
    "        true_values.iloc[i], residuals_lin.iloc[i], fmt='o',\n",
    "        color=color, markersize=22, markeredgecolor='black', capsize=24, linewidth=2)\n",
    "ax1.axhline(0, color='red', linestyle='--', linewidth=1, zorder=1)\n",
    "\n",
    "ax1.text(0.05, 0.84, f\"$\\\\sigma_{{FDS}} = {sigma_lin:.4f}$\", fontsize=20, color='blue',\n",
    "         bbox=dict(facecolor='white', edgecolor='black', alpha=0.7), transform=ax1.transAxes)\n",
    "\n",
    "ax1.text(0.53, 0.84, f\"$mean = {lin_mean:.4f}$\", fontsize=20, color='red',\n",
    "         bbox=dict(facecolor='white', edgecolor='black', alpha=0.7), transform=ax1.transAxes)\n",
    "\n",
    "ax1.text(0.51, 0.64, f\"$median = {lin_median:.4f}$\", fontsize=20, color='red',\n",
    "         bbox=dict(facecolor='white', edgecolor='black', alpha=0.7), transform=ax1.transAxes)\n",
    "ax1.set_xlim(0.03, 4.5)\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_ylim(-0.8, 0.8)\n",
    "ax1.set_yscale('linear') \n",
    "ax1.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "ax1.tick_params(axis='both', which='major', labelsize=24)\n",
    "ax1.set_ylabel(r'$\\frac{\\mathrm{Equation 1 (au)} - \\mathrm{Horizons (au)}}{\\mathrm{Horizons (au)}}$', fontsize=24)\n",
    "ax1.set_xlabel('Horizons (au)', fontsize = 20)\n",
    "\n",
    "# Bottom-left: Residuals for Linear fit\n",
    "ax2 = plt.subplot(gs[0, 2])\n",
    "\n",
    "for i, color in enumerate(colors):\n",
    "    ax2.errorbar(true_values.iloc[i], residuals_Horizon_FindOrb.iloc[i], fmt='o', color=color, markersize=22, markeredgecolor='black', capsize=24, linewidth=2)\n",
    "\n",
    "ax2.axhline(0, color='red', linestyle='--', linewidth=1, zorder=1)\n",
    "\n",
    "ax2.text(0.05, 0.84, f\"$\\\\sigma_{{FDS}} = {sigma_Horizon_FindOrb:.4f}$\", fontsize=20, color='blue', bbox=dict(facecolor='white', edgecolor='black', alpha=0.7), transform=ax2.transAxes)\n",
    "\n",
    "ax2.text(0.53, 0.84, f\"$mean = {Horizon_FindOrb_mean:.4f}$\", fontsize=20, color='red',\n",
    "         bbox=dict(facecolor='white', edgecolor='black', alpha=0.7), transform=ax2.transAxes)\n",
    "\n",
    "ax2.text(0.51, 0.64, f\"$median = {Horizon_FindOrb_median:.4f}$\", fontsize=20, color='red',\n",
    "         bbox=dict(facecolor='white', edgecolor='black', alpha=0.7), transform=ax2.transAxes)\n",
    "\n",
    "ax2.set_xlim(0.03, 4.5)\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_ylim(-0.8, 0.8)\n",
    "ax2.set_yscale('linear')\n",
    "ax2.set_yticklabels([])\n",
    "ax2.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "ax2.set_ylabel(r'$\\frac{\\mathrm{Find\\_Orb (au)} - \\mathrm{Horizons (au)}}{\\mathrm{Horizons (au)}}$', fontsize=24)\n",
    "ax2.tick_params(axis='both', which='major', labelsize=24)\n",
    "ax2.set_xlabel('Horizons (au)', fontsize = 20)\n",
    "\n",
    "# Bottom-right: Residuals for Polynomial fit\n",
    "ax3 = plt.subplot(gs[0, 1])\n",
    "for i, color in enumerate(colors):\n",
    "    ax3.errorbar(true_values.iloc[i], residuals_poly.iloc[i], fmt='o', color=color, markersize=22, markeredgecolor='black', capsize=24, linewidth=2)\n",
    "ax3.axhline(0, color='red', linestyle='--', linewidth=1, zorder=1)\n",
    "\n",
    "ax3.text(0.05, 0.84, f\"$\\\\sigma_{{FDS}} = {sigma_poly:.4f}$\", fontsize=20, color='blue',\n",
    "         bbox=dict(facecolor='white', edgecolor='black', alpha=0.7), transform=ax3.transAxes)\n",
    "\n",
    "ax3.text(0.53, 0.84, f\"$mean = {poly_mean:.4f}$\", fontsize=20, color='red',\n",
    "         bbox=dict(facecolor='white', edgecolor='black', alpha=0.7), transform=ax3.transAxes)\n",
    "\n",
    "ax3.text(0.51, 0.64, f\"$median = {poly_median:.4f}$\", fontsize=20, color='red',\n",
    "         bbox=dict(facecolor='white', edgecolor='black', alpha=0.7), transform=ax3.transAxes)\n",
    "ax3.set_xticklabels([])\n",
    "ax3.set_xlim(0.03, 4.5)\n",
    "ax3.set_xscale('log')\n",
    "ax3.set_ylim(-0.8, 0.8)\n",
    "ax3.set_yscale('linear')\n",
    "ax3.set_yticklabels([])\n",
    "ax3.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "ax3.tick_params(axis='both', which='major', labelsize=24)\n",
    "ax3.set_ylabel(r'$\\frac{\\mathrm{Equation 5 (au)} - \\mathrm{Horizons (au)}}{\\mathrm{Horizons (au)}}$', fontsize=24)\n",
    "ax3.set_xlabel('Horizons (au)', fontsize = 20)\n",
    "\n",
    "# --- Linear residuals ---\n",
    "#ax1.set_title(\"Residuals: Equation 1 (Linear)\", fontsize=24, pad=20)\n",
    "\n",
    "# --- Polynomial residuals ---\n",
    "#ax3.set_title(\"Residuals: Equation 5 (Polynomial)\", fontsize=24, pad=20)\n",
    "\n",
    "# --- Find_Orb residuals ---\n",
    "#ax2.set_title(\"Residuals: Find_Orb vs Horizons\", fontsize=24, pad=20)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Sorting Asteroid distance in legend\n",
    "if isinstance(true_values, pd.Series):\n",
    "    results_df['Mean Delta'] = true_values\n",
    "else:\n",
    "    results_df['Mean Delta'] = true_values.mean(axis=1)\n",
    "\n",
    "sorted_indices = results_df['Mean Delta'].values.argsort()\n",
    "sorted_file_names = results_df['File Name'].iloc[sorted_indices]\n",
    "sorted_deltas = results_df['Mean Delta'].iloc[sorted_indices]\n",
    "colors = np.array(colors)\n",
    "sorted_colors = colors[sorted_indices]\n",
    "\n",
    "max_name_length = max(len(file_name.replace('.csv', '')) for file_name in sorted_file_names)\n",
    "handles = [Line2D([0], [0], marker='o', color='w', markerfacecolor=color,\n",
    "           markeredgecolor='black', markersize=15,\n",
    "           label=f\"{file_name.replace('.csv', '').ljust(max_name_length)}\")\n",
    "           for file_name, delta, color in zip(sorted_file_names, sorted_deltas, sorted_colors)]\n",
    "\n",
    "fig.legend(handles=handles, loc='center left', bbox_to_anchor=(0.85, 0.5), fontsize=16,\n",
    "           title=\"Asteroids \\n (Multiple nights)\", title_fontsize=18, frameon=True)\n",
    "\n",
    "plt.subplots_adjust(left=0.05, right=0.84, top=0.93, bottom=0.08, wspace=0.20, hspace=0.05)\n",
    "\n",
    "plt.savefig(\"Figure 8 (row 2).png\", format=\"png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f64168-2269-4a95-bb3f-1f47864a7638",
   "metadata": {},
   "source": [
    "##### Step 19: Convert the Horizons ephemeris data into Find_Orb compatible .obs file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1189e0-a961-459e-a989-b0c12a09f1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following constants define the Minor Planet Center (MPC) fixed-column format.\n",
    "\n",
    "LINE_LEN = 80   # Each MPC observation line must be exactly 80 characters long (columns)\n",
    "\n",
    "COL_DATE = 14   # Column where the date string starts: \"CYYYY MM DDD.ddddd\" (17 characters)\n",
    "COL_RA   = 32   # Column where RA starts: \"HH MM SS.ss\" (11 characters)\n",
    "COL_DEC  = 47   # Column where DEC starts: \"+DD MM SS.s\" (11 characters)\n",
    "COL_OBS  = 77   # Column where the observatory code starts (3 characters)\n",
    "\n",
    "def _put_at(buf, text, col_start_1based):\n",
    "    \n",
    "    # Inserts a string into a character buffer at a fixed 1-based column position.\n",
    "    i = col_start_1based - 1          # 1-based column index converted to 0-based index.\n",
    "    buf[i:i+len(text)] = list(text)   # buffer characters overwritten with the given text\n",
    "\n",
    "def _ra_hms(ra_deg, prec=6):\n",
    "    \n",
    "    # RA in the Horizons file is converted from degrees to hours-minutes-seconds (HMS) for Find_Orb. \n",
    "    return Angle(float(ra_deg)*u.deg).to_string(\n",
    "        unit=u.hour,                  # degrees → hours\n",
    "        sep=' ',                      # HMS seperated with spaces\n",
    "        precision=prec,               # Number of decimal places for seconds\n",
    "        pad=True                      # Pad with leading zeros\n",
    "    )\n",
    "\n",
    "def _dec_dms(dec_deg, prec=5):\n",
    "    \n",
    "    # DEC in the Horizons file is converted from degrees to signed degrees-minutes-seconds (DMS) for Find_Orb.\n",
    "    s = Angle(float(dec_deg)*u.deg).to_string(\n",
    "        unit=u.deg,                   # units in degrees\n",
    "        sep=' ',                      # DMS seperated with spaces\n",
    "        precision=prec,               # Number of decimal places for arcseconds\n",
    "        pad=True,                     # Pad with leading zeros\n",
    "        alwayssign=True               # Always include + or - sign\n",
    "    )\n",
    "    \n",
    "    return ' '.join(s.split())        # Normalize spacing to single spaces\n",
    "\n",
    "def _date_from_jd(jd_utc):\n",
    "    \n",
    "    # Converts Julian Date (UTC) to MPC format\n",
    "    t  = Time(float(jd_utc), format='jd', scale='utc')  # Create Astropy Time\n",
    "    dt = t.to_datetime()                                # Convert to Python datetime\n",
    "\n",
    "    # Compute fractional day required by MPC format\n",
    "    day_frac = dt.day + (dt.hour + dt.minute / 60 + dt.second / 3600 + dt.microsecond / 1e6 / 3600) / 24.0\n",
    "\n",
    "    # date formatted as: CYYYY MM DDD.ddddd.\n",
    "    return f\"C{dt.year:04d} {dt.month:02d} {day_frac:08.5f}\"\n",
    "\n",
    "# Builds the single 80-character MPC observation line.\n",
    "def _format_line(jd, ra_deg, dec_deg, observatory='807'):\n",
    "\n",
    "    # Initialize blank 80-character buffer. \n",
    "    line = [' '] * LINE_LEN          \n",
    "\n",
    "    date_str = _date_from_jd(jd)                   # MPC-formatted date (17 chars)\n",
    "    ra_str   = _ra_hms(ra_deg, prec=6).rjust(11)   # RA formatted and right-justified\n",
    "    dec_str  = _dec_dms(dec_deg, prec=5).rjust(11) # Dec formatted and right-justified\n",
    "    obs_str  = f\"{observatory:>3}\"                 # Observatory code, right-justified\n",
    "\n",
    "    # Insert formatted fields into fixed MPC columns\n",
    "    _put_at(line, date_str, COL_DATE)\n",
    "    _put_at(line, ra_str,   COL_RA)\n",
    "    _put_at(line, dec_str,  COL_DEC)\n",
    "    _put_at(line, obs_str,  COL_OBS)\n",
    "\n",
    "    # Convert character buffer to string\n",
    "    return ''.join(line)             \n",
    "\n",
    "def _normalize_cols(cols):\n",
    "    # Normalizes DataFrame column names to avoid hidden Unicode / spacing issues. \n",
    "    fixed = []\n",
    "    \n",
    "    for c in cols:\n",
    "        s = unicodedata.normalize('NFKC', str(c))  # Normalize Unicode characters\n",
    "        s = s.replace('\\u00A0', ' ')               # Replace non-breaking spaces\n",
    "        s = ' '.join(s.split())                    # Collapse multiple spaces\n",
    "        s = s.strip()                              # Remove leading/trailing spaces\n",
    "        fixed.append(s)\n",
    "        \n",
    "    return fixed\n",
    "\n",
    "# output directory for .obs files\n",
    "formatted_folder  = os.path.join(asteroids_folder, \"formatted_obs\")\n",
    "os.makedirs(formatted_folder, exist_ok=True)\n",
    "\n",
    "# Loop over all .csv files in the folder. \n",
    "for fn in os.listdir(asteroids_folder):\n",
    "    \n",
    "    # Files without .csv extensions are skipped. \n",
    "    if not fn.lower().endswith(\".csv\"):\n",
    "        continue                                 \n",
    "\n",
    "    path = os.path.join(asteroids_folder, fn)\n",
    "    df = pd.read_csv(path)                       \n",
    "\n",
    "    # Normalize column names to avoid parsing issues\n",
    "    df.columns = _normalize_cols(df.columns)\n",
    "\n",
    "    # Drop any completely blank column names\n",
    "    blank_cols = [c for c in df.columns if c == \"\"]\n",
    "    if blank_cols:\n",
    "        df = df.drop(columns=blank_cols)\n",
    "\n",
    "    try:\n",
    "        \n",
    "        # Expected column names\n",
    "        jd_col  = 'Date__(UT)__HR:MN'\n",
    "        ra_col  = 'R.A.__(ICRF)'\n",
    "        dec_col = 'DEC__(ICRF)'\n",
    "\n",
    "        # Fallback search if RA column name does not match exactly\n",
    "        if ra_col not in df.columns:\n",
    "            \n",
    "            candidates = [c for c in df.columns if 'R.A.' in c or 'RA' in c.upper()]\n",
    "            if candidates:\n",
    "                ra_col = candidates[0]\n",
    "            else:\n",
    "                raise KeyError(\"RA column not found after normalization.\")\n",
    "\n",
    "        # Fallback search if DEC column name does not match exactly\n",
    "        if dec_col not in df.columns:\n",
    "            candidates = [c for c in df.columns if 'DEC' in c.upper()]\n",
    "            if candidates:\n",
    "                dec_col = candidates[0]\n",
    "            else:\n",
    "                raise KeyError(\"DEC column not found after normalization.\")\n",
    "\n",
    "        # Convert columns to numeric values\n",
    "        jd  = pd.to_numeric(df[jd_col],  errors=\"coerce\")\n",
    "        ra  = pd.to_numeric(df[ra_col],  errors=\"coerce\")\n",
    "        dec = pd.to_numeric(df[dec_col], errors=\"coerce\")\n",
    "\n",
    "        # Keep only rows with valid numeric JD, RA, and DEC. \n",
    "        w   = pd.DataFrame({\"jd\": jd, \"ra\": ra, \"dec\": dec}).dropna()\n",
    "\n",
    "        if w.empty:\n",
    "            print(f\"No numeric rows in {fn}; skipped.\")\n",
    "            continue\n",
    "\n",
    "        # Format each row into MPC .obs line\n",
    "        lines = [\n",
    "            _format_line(jd, ra, dec, observatory=\"807\")\n",
    "            for jd, ra, dec in w[[\"jd\",\"ra\",\"dec\"]].itertuples(index=False, name=None)]\n",
    "\n",
    "        # Output file as a .obs file extension. \n",
    "        out = os.path.join(formatted_folder, f\"{os.path.splitext(fn)[0]}.obs\")\n",
    "        with open(out, \"w\") as f:\n",
    "            f.write(\"\\n\".join(lines) + \"\\n\")\n",
    "        print(f\"Saved {out}\")\n",
    "\n",
    "    # Report errors\n",
    "    except Exception as e:\n",
    "        print(f\"Error in {fn}: {e}\\nColumns seen: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be51a46-26fc-4f89-bcbc-f491788a090f",
   "metadata": {},
   "source": [
    "##### Step 20: Print all 10 asteroids' MPC-formatted .obs files here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43da34c2-fafd-475c-a8b7-549ae8d1974b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in sorted(os.listdir(formatted_folder)):\n",
    "    if file_name.endswith('.obs'):\n",
    "        print(f\"\\n📄 {file_name}\")\n",
    "        \n",
    "        with open(os.path.join(formatted_folder, file_name), 'r') as f:\n",
    "            print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace8e380-0038-4291-84e6-981e230e056b",
   "metadata": {},
   "source": [
    "#### **Summary**: \n",
    "\n",
    "##### 1) This notebook processes multiple-night Horizons data for 10 asteroids to measure their distances using the topocentric parallax method described in Equations 1 and 5 of the manuscript.\n",
    "\n",
    "##### 2) For each asteroid, we fit both linear and quadratic parallax models and compare the recovered distances to the true Horizons distances. We further compute residuals, percentage errors, and χ² statistics. In addition, in Steps 19 and 20 , we convert the asteroid astrometric measurements stored in csv files into Minor Planet Center (MPC)–compliant 80-column .obs files for use with Find_Orb. This process enforces the required fixed-width formatting, converts JD timestamps into MPC fractional-day calendar dates, and transforms RA and DEC from degrees into HMS and DMS representations, respectively.\n",
    "\n",
    "##### 3) The quadratic parallax model consistently yields a reduced χ² close to unity, indicating that it accurately captures the parallax signal over long observational arcs. In contrast, the linear model produces significantly larger χ² values, indicating unnecessary model complexity.\n",
    "\n",
    "##### 4) Overall, these results demonstrate that a simple quadratic topocentric parallax fit is robust and reliable for long observational windows. For Find_Orb, each valid measurement is written as a single 80-character MPC observation line with the appropriate observatory code, producing .obs files that can be ingested directly by orbit determination software for distance estimation and orbit fitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
